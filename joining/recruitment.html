---
layout: defaultpapers
title: Recruitment
subtitle: 
description: 
featured_image: /images/monstera3.jpg
---

<body>
  <section class="intro">
 
    <div class="wrap">
      <h1>{{ page.title }}</h1>
      <p>{{ page.subtitle }}</p>
    </div>
  </section>

  <section class="portfolio">
    <div class="wrap">
      <h2>Background: Neural representation of shared concepts</h2>
      <p>‘Common ground’ is all the stuff you implicitly rely on to understand each other in conversation. For example, you might tell a friend to meet you in ‘the stats room’ - but which room it is, what building it’s in, when you took the course, etc. are all assumed to be shared knowledge implied by the reference, even if you never explicitly said ‘we should call this the stats room’. And importantly, you couldn’t pull this off with a complete stranger! 
<p>
In this project, we study how two people develop common ground -- specifically, how their neural representations (encodings) of concepts and language shift over the course of a conversation. We asked participants to play a communication game in the fMRI scanner, where the ‘speaker’ has to describe an abstract tangram shape to their partner, the ‘listener’, who has to guess which one is being referred to. Related work from our lab that uses the same game can be found <a href="https://psycnet.apa.org/manuscript/2022-53084-001.pdf"> here </a> and <a href="https://osf.io/preprints/psyarxiv/4fpr3">here </a>. This method of fMRI data collection involves the use of hyperscanning - simultaneously and concurrently collecting the brain activity of both players during naturalistic communication. <p>

      <h2>Project: Alignment of fMRI recordings and spoken conversation</h2>
      <p>‘In order to more accurately identify the brain networks involved in generating and coordinating references with a specific conversational partner, we are looking for a research assistant that will help us connect the data collected from the fMRI scanner to the spoken conversation. The conversations have already been transcribed using a speech-to-text model called <a href="https://openai.com/research/whisper">WHISPER</a>, but the transcripts need to be corrected and aligned to the time series from the scanner. There is possibility for this role to be extended into the coming school year, working with the resulting data or on related projects.<p> <p>

<b>Relevant skills:</b> experience with fMRI data, experience with conversational corpora, linguistics knowledge, keen eye for detail. It’s a bonus if you already have some of these skills, but don’t hesitate to apply if you don’t yet have these skills and want an opportunity to develop them. 
<p>
<b> How to apply: </b> Please send an email to Yuliya (<a href="mailto:yzubak@wisc.edu">yzubak@wisc.edu</a>) with your CV, unoffical transcript, and a brief description of: <p>(1) what interests you about this project, <p>(2) other aspects of language and interaction you’re interested in researching, and <p>(3) your relevant expertise. <p> <p>Thank you for your interest in the Social Interaction Lab!

<br><br><br>
    </div>
  </section>
</body>
